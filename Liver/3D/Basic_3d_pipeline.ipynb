{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "# from models import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.insert(0,'/home/roshansk/Covid/CXRData/')\n",
    "from SegLearner import *\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "sys.path.insert(0,'/home/roshansk/Covid/RibFrac/Models/')\n",
    "from models import *\n",
    "\n",
    "from monai.transforms import AddChannel, AdjustContrast, Resize, LoadNifti, LoadNiftiD, Compose, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/home/roshansk/Covid/3dSeg/Liver/3D/3D_UNet/pytorch_3D-Unet-master/unet3d/')\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFolder = '/home/roshansk/Covid/3dSeg/Liver/Data/OrigData/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgPath</th>\n",
       "      <th>maskPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "      <td>/home/roshansk/Covid/3dSeg/Liver/Data/OrigData...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             imgPath  \\\n",
       "0  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...   \n",
       "1  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...   \n",
       "2  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...   \n",
       "3  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...   \n",
       "4  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...   \n",
       "\n",
       "                                            maskPath  \n",
       "0  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...  \n",
       "1  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...  \n",
       "2  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...  \n",
       "3  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...  \n",
       "4  /home/roshansk/Covid/3dSeg/Liver/Data/OrigData...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFiles = os.listdir(trainFolder)\n",
    "imgFiles = [x for x in dataFiles if 'orig' in x]\n",
    "maskFiles = [x for x in dataFiles if 'liver' in x]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'imgPath':imgFiles, 'maskPath':maskFiles})\n",
    "df.imgPath = df.imgPath.apply(lambda x : os.path.join(trainFolder, x))\n",
    "df.maskPath = df.maskPath.apply(lambda x : os.path.join(trainFolder, x))\n",
    "\n",
    "\n",
    "## Train Test Split\n",
    "\n",
    "trainDf = df.iloc[:16]\n",
    "testDf = df.iloc[16:]\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SegDataset3D(torch.utils.data.Dataset):\n",
    "    \n",
    "  def __init__(self, df, imgTransforms = None, maskTransforms = None, preload = False, imgSize = 256):\n",
    "\n",
    "\n",
    "    self.df = df\n",
    "    self.transforms = transforms\n",
    "    self.preload = preload\n",
    "    self.imgSize = imgSize\n",
    "    self.imgTransforms = imgTransforms\n",
    "    self.maskTransforms = maskTransforms\n",
    "    \n",
    "    if self.preload:\n",
    "        self.preloadData()\n",
    "    \n",
    "    \n",
    "  def preloadData(self):\n",
    "    imgList = []\n",
    "    maskList = []\n",
    "    \n",
    "    for i in tqdm(range(len(self.df))):\n",
    "            \n",
    "            img = PIL.Image.open(self.df.iloc[i]['imgPath'])\n",
    "\n",
    "            img = torchvision.transforms.Resize( (self.imgSize, self.imgSize))(img)\n",
    "            imgList.append(np.array(img))\n",
    "\n",
    "            \n",
    "            mask = PIL.Image.open(self.df.iloc[i]['maskPath'])\n",
    "\n",
    "            mask = torchvision.transforms.Resize( (self.imgSize, self.imgSize))(mask)\n",
    "            maskList.append(np.array(mask))\n",
    "\n",
    "            \n",
    "    self.imgData = np.array(imgList)\n",
    "    self.maskData = np.array(maskList)\n",
    "    del imgList, maskList\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        \n",
    "    if self.preload:\n",
    "        img = self.imgData[index,:,:,:]\n",
    "        img = PIL.Image.fromarray(img)\n",
    "        \n",
    "        mask = self.maskData[index,:,:,:]\n",
    "        mask = PIL.Image.fromarray(mask)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        imgFilename = self.df.iloc[index]['imgPath']\n",
    "        maskFilename = self.df.iloc[index]['maskPath'] \n",
    "        \n",
    "        img = LoadNifti(image_only=True)(imgFilename)\n",
    "        mask = LoadNifti(image_only=True)(maskFilename)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    if self.imgTransforms:\n",
    "        img = self.imgTransforms(img)\n",
    "    \n",
    "    if self.maskTransforms:\n",
    "        mask = self.maskTransforms(mask)\n",
    "        \n",
    "\n",
    "#     if len(mask.shape)==4:\n",
    "#         mask = mask[0,:,:,:]\n",
    "    \n",
    "#     if len(img.shape)==4:\n",
    "#         img = img[0,:,:,:]\n",
    "\n",
    "    mask[mask>0] = 1\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def evalModel( model, testLoader, threshold = 0.5):\n",
    "        \n",
    "    \"\"\"\n",
    "    Evaluation of the model is done using Dice metric\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    diceScores = []\n",
    "\n",
    "    for (img, mask) in tqdm(testLoader):\n",
    "\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "\n",
    "        out = model(img)\n",
    "        out = out>threshold\n",
    "        \n",
    "        \n",
    "        out = out.detach().cpu().numpy()\n",
    "        mask = mask.detach().cpu().numpy()\n",
    "        \n",
    "        diceScores.append(dice(out, mask))\n",
    "        \n",
    "        \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return np.mean(diceScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 3\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "imgSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgTransforms = Compose([AddChannel(), Resize((imgSize,imgSize,imgSize)), ToTensor()])\n",
    "maskTransforms = Compose([AddChannel(), Resize((imgSize,imgSize,imgSize)), ToTensor()])\n",
    "\n",
    "trainDataset = SegDataset3D(trainDf, imgTransforms, maskTransforms, preload=False)\n",
    "testDataset = SegDataset3D(testDf, imgTransforms, maskTransforms, preload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batchSize, \n",
    "                                          shuffle=True, num_workers=6)\n",
    "\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=1, \n",
    "                                          shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(in_channels=1, out_channels=1, final_sigmoid = True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=lr),\n",
    "])\n",
    "\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 0.1314698855082194 Val IOU : 0.5716367552811221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 0.11820024996995926 Val IOU : 0.5170598785692917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0.0\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     self.epoch +=1\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    model.training = False\n",
    "    \n",
    "    for data,mask in trainLoader:\n",
    "        data,mask = data.to(device), mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, mask.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss/len(trainLoader)\n",
    "            \n",
    "    epochDice = evalModel(model, testLoader)\n",
    "    \n",
    "    print(f\"Epoch : {epoch} Loss : {epoch_loss} Val IOU : {epochDice}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
